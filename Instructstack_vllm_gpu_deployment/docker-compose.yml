version: '3.8'

# Environment variables for VLLM configuration
# You can override these values by creating a .env file or setting them in your shell
x-vllm-config: &vllm-config
  # VLLM Server Configuration
  MAX_NUM_SEQS: 10                    # Maximum number of sequences to process concurrently
  VLLM_PORT: 8000                     # Port for VLLM server to listen on
  GPU_MEMORY_UTILIZATION: 0.3         # GPU memory utilization (0.0 to 1.0)
  MAX_MODEL_LEN: 2048                 # Maximum model length for tokenization
  
  # GPU Configuration
  NVIDIA_VISIBLE_DEVICES: 0           # GPU device ID to use (0 for first GPU)
  
  # Model Configuration
  DEFAULT_MODEL: yasserrmd/Text2SQL-1.5B  # Default model to load
  
  # Logfire Configuration
  # Don't use this key this is wrong key use your own key
  LOGFIRE_TOKEN: aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  # Replace with your Logfire serve key here

services:
  # VLLM Server - Main instance
  vllm:
    image: vllm-gpu-image:latest
    container_name: ${VLLM_CONTAINER_NAME:-vllm_server}
    runtime: nvidia
    volumes:
      - ./models:/models  # Mount local models directory to container
    environment:
      DEFAULT_MODEL: ${DEFAULT_MODEL:-yasserrmd/Text2SQL-1.5B}
      PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True  # Optimize CUDA memory allocation
      NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES:-0}
      MAX_NUM_SEQS: ${MAX_NUM_SEQS:-10}
      VLLM_PORT: ${VLLM_PORT:-8000}
      GPU_MEMORY_UTILIZATION: ${GPU_MEMORY_UTILIZATION:-0.3}

    command: >
      bash -c "source /opt/conda/etc/profile.d/conda.sh &&
               conda activate vllm_env &&
               vllm serve /models/$$DEFAULT_MODEL --max-num-seqs $$MAX_NUM_SEQS --port $$VLLM_PORT --gpu-memory-utilization $$GPU_MEMORY_UTILIZATION"
               
    ports:
      - "${VLLM_PORT:-8000}:${VLLM_PORT:-8000}"
    networks:
      - ${NETWORK_NAME:-vllm-net}
    restart: "no"  # Container is removed unless you override with --rm

  # VLLM Server - Secondary instance with different configuration
  # vllm1:
  #   image: hamzaak4/vllm-gpu-image:latest
  #   container_name: ${VLLM1_CONTAINER_NAME:-vllm_server1}
  #   runtime: nvidia
    
  #   volumes:
  #     - ./models:/models
  #   environment:
  #     DEFAULT_MODEL: ${DEFAULT_MODEL_1:-premai-io/prem-1B-SQL}
  #     PYTORCH_CUDA_ALLOC_CONF: expandable_segments:True
  #     NVIDIA_VISIBLE_DEVICES: ${NVIDIA_VISIBLE_DEVICES_1:-0}
  #     MAX_NUM_SEQS: ${MAX_NUM_SEQS_1:-1}
  #     VLLM_PORT: ${VLLM_PORT_1:-8001}
  #     GPU_MEMORY_UTILIZATION: ${GPU_MEMORY_UTILIZATION_1:-0.3}
  #     MAX_MODEL_LEN: ${MAX_MODEL_LEN:-2048}

  #   command: >
  #     bash -c "source /opt/conda/etc/profile.d/conda.sh &&
  #         conda activate vllm_env &&
  #         vllm serve /models/$$DEFAULT_MODEL --max-num-seqs $$MAX_NUM_SEQS --port $$VLLM_PORT --gpu-memory-utilization $$GPU_MEMORY_UTILIZATION"
               
  #   ports:
  #     - "${VLLM_PORT_1:-8001}:${VLLM_PORT_1:-8001}"
  #   networks:
  #     - ${NETWORK_NAME:-vllm-net}
  #   restart: "no"

  # FastAPI Web Application - Provides REST API interface to VLLM
  fastapi_app:
    image: fastapi-vllm:Latest
    container_name: ${FASTAPI_CONTAINER_NAME:-fastapi-app}
    ports:
      - "9000:9000"
    volumes:
      - ./models:/models
      - /var/run/docker.sock:/var/run/docker.sock  # Required for Docker API access
    environment:
      LOGFIRE_TOKEN: ${LOGFIRE_TOKEN}  # Your Logfire serve key for logging
      VLLM_API_URL: ${VLLM_API_URL:-http://vllm:8000/v1/completions}  # Primary VLLM API endpoint
      # VLLM_API_URL: ${VLLM_API_URL_1:-http://vllm1:8001/v1/completions}  # Secondary VLLM API endpoint
    depends_on:
      - vllm
    networks:
      - ${NETWORK_NAME:-vllm-net}

  # Prometheus - Metrics collection and monitoring
  prometheus:
    image: prom/prometheus
    container_name: ${PROMETHEUS_CONTAINER_NAME:-prometheus}
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports:
      - "9090:9090"
    networks:
      - ${NETWORK_NAME:-vllm-net}

  # Node Exporter - System metrics collection
  node_exporter:
    image: prom/node-exporter
    container_name: ${NODE_EXPORTER_CONTAINER_NAME:-node_exporter}
    ports:
      - "9100:9100"
    networks:
      - ${NETWORK_NAME:-vllm-net}

  # Grafana - Metrics visualization and dashboards
  grafana:
    image: grafana/grafana
    container_name: ${GRAFANA_CONTAINER_NAME:-grafana}
    ports:
      - "3000:3000"
    networks:
      - ${NETWORK_NAME:-vllm-net}
    volumes:
      - grafana-storage:/var/lib/grafana  # Persistent storage for Grafana data

  # cAdvisor - Container resource usage monitoring
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: ${CADVISOR_CONTAINER_NAME:-cadvisor}
    ports:
      - "8081:8080"
    privileged: true  # Required for system-level access
    volumes:
      - /:/rootfs:ro                    # Read-only access to root filesystem
      - /var/run:/var/run:ro            # Docker runtime information
      - /sys:/sys:ro                    # System information
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/lib/docker/image:/var/lib/docker/image:ro
      - /var/lib/docker/overlay2:/var/lib/docker/overlay2:ro
    networks:
      - ${NETWORK_NAME:-vllm-net}

  # NVIDIA DCGM Exporter - GPU metrics collection
  dcgm-exporter:
    image: nvidia/dcgm-exporter:latest
    container_name: ${DCGM_EXPORTER_CONTAINER_NAME:-dcgm_exporter}
    runtime: nvidia
    ports:
      - "9400:9400"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all  # Monitor all available GPUs
    networks:
      - ${NETWORK_NAME:-vllm-net}

# Network configuration for inter-service communication
networks:
  vllm-net:
    driver: bridge

# Persistent volumes for data storage
volumes:
  grafana-storage:  # Grafana dashboards and configuration persistence    
