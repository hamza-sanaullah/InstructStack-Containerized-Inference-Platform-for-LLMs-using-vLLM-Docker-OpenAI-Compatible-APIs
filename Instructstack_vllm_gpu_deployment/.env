# VLLM Configuration - Copy these variables to a .env file and modify the values as needed

# Primary VLLM Server Configuration
MAX_NUM_SEQS=10                    # Maximum number of sequences to process concurrently
VLLM_PORT=8000                     # Port for VLLM server to listen on
GPU_MEMORY_UTILIZATION=0.3         # GPU memory utilization (0.0 to 1.0)
MAX_MODEL_LEN=2048                 # Maximum model length for tokenization
NVIDIA_VISIBLE_DEVICES=0           # GPU device ID to use (0 for first GPU)
DEFAULT_MODEL=yasserrmd/Text2SQL-1.5B  # Default model to load

# Secondary VLLM Server Configuration (vllm1 service)
MAX_NUM_SEQS_1=1                   # Different sequence limit for secondary server
VLLM_PORT_1=8001                   # Different port for secondary server
GPU_MEMORY_UTILIZATION_1=0.3       # GPU memory utilization for secondary server
NVIDIA_VISIBLE_DEVICES_1=0         # GPU device for secondary server
DEFAULT_MODEL_1=premai-io/prem-1B-SQL  # Different model for secondary server

# FastAPI Configuration
LOGFIRE_TOKEN=pdhsshhfjsjjsjsjjjjjjjj   # Replace with your Logfire serve key here
VLLM_API_URL=http://vllm:8000/v1/completions  # Primary VLLM API endpoint
# VLLM_API_URL_1=http://vllm1:8001/v1/completions  # Secondary VLLM API endpoint (uncomment to use)

# Optional: Override default values for specific use cases
# MAX_NUM_SEQS=20                  # Increase for higher throughput
# GPU_MEMORY_UTILIZATION=0.5       # Increase for better performance (if GPU memory allows)
# NVIDIA_VISIBLE_DEVICES=1         # Use second GPU if available

# Model Download Configuration
MODEL_REPO_ID=premai-io/prem-1B-SQL  # Default model to download
MODEL_LOCAL_DIR=models/premai-io/prem-1B-SQL  # Local directory to save model
MODEL_USE_SYMLINKS=False              # Whether to use symlinks (False for Docker compatibility)
MODELS_TO_DOWNLOAD=                   # Comma-separated list for multiple models (e.g., "yasserrmd/Text2SQL-1.5B,premai-io/prem-1B-SQL")
