# VLLM Configuration - Copy these variables to a .env file and modify the values as needed

# Primary VLLM Server Configuration
MAX_NUM_SEQS=10                    # Maximum number of sequences to process concurrently
VLLM_PORT=8000                     # Port for VLLM server to listen on
GPU_MEMORY_UTILIZATION=0.3         # GPU memory utilization (0.0 to 1.0)
MAX_MODEL_LEN=2048                 # Maximum model length for tokenization
NVIDIA_VISIBLE_DEVICES=0           # GPU device ID to use (0 for first GPU)
DEFAULT_MODEL=yasserrmd/Text2SQL-1.5B  # Default model to load

# Secondary VLLM Server Configuration (vllm1 service)
MAX_NUM_SEQS_1=1                   # Different sequence limit for secondary server
VLLM_PORT_1=8001                   # Different port for secondary server
GPU_MEMORY_UTILIZATION_1=0.3       # GPU memory utilization for secondary server
NVIDIA_VISIBLE_DEVICES_1=0         # GPU device for secondary server
DEFAULT_MODEL_1=premai-io/prem-1B-SQL  # Different model for secondary server

# FastAPI Configuration
# Don't use this key this is wrong key use your own key
LOGFIRE_TOKEN=aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  # Replace with your Logfire serve key here
VLLM_API_URL=http://vllm:8000/v1/completions  # Primary VLLM API endpoint
# VLLM_API_URL_1=http://vllm1:8001/v1/completions  # Secondary VLLM API endpoint (uncomment to use)

# Optional: Override default values for specific use cases
# MAX_NUM_SEQS=20                  # Increase for higher throughput
# GPU_MEMORY_UTILIZATION=0.5       # Increase for better performance (if GPU memory allows)
# NVIDIA_VISIBLE_DEVICES=1         # Use second GPU if available

# Model Download Configuration
MODEL_REPO_ID=premai-io/prem-1B-SQL  # Default model to download
MODEL_LOCAL_DIR=models/premai-io/prem-1B-SQL  # Local directory to save model
MODEL_USE_SYMLINKS=False              # Whether to use symlinks (False for Docker compatibility)
MODELS_TO_DOWNLOAD=                   # Comma-separated list for multiple models (e.g., "yasserrmd/Text2SQL-1.5B,premai-io/prem-1B-SQL")

# Container Naming Configuration
# These variables define the names of containers for consistent naming across all configurations
# Note: Service names and network names in docker-compose.yml must remain static and cannot use environment variables

# Container Names (used in docker-compose.yml container_name and prometheus.yml targets)
VLLM_CONTAINER_NAME=vllm_server     # Primary VLLM container name
VLLM1_CONTAINER_NAME=vllm_server1   # Secondary VLLM container name
FASTAPI_CONTAINER_NAME=fastapi-app  # FastAPI container name
PROMETHEUS_CONTAINER_NAME=prometheus  # Prometheus container name
NODE_EXPORTER_CONTAINER_NAME=node_exporter  # Node exporter container name
GRAFANA_CONTAINER_NAME=grafana      # Grafana container name
CADVISOR_CONTAINER_NAME=cadvisor    # cAdvisor container name
DCGM_EXPORTER_CONTAINER_NAME=dcgm_exporter  # DCGM exporter container name

# Concurrency Test Configuration
# These variables can be used with the concurrency_test.py script
VLLM_API_URL_TEST=http://localhost:8000/v1/completions  # API URL for testing (localhost for external testing)
VLLM_MODEL_TEST=/models/yasserrmd/Text2SQL-1.5B  # Model path for testing
CONCURRENCY=10                      # Number of concurrent users for testing
REQUESTS_PER_CLIENT=5              # Requests per user for testing
REQUEST_TIMEOUT=30                  # Request timeout in seconds for testing
MAX_TOKENS=128                     # Maximum tokens in response for testing
TEMPERATURE=0.3                    # Response temperature for testing
