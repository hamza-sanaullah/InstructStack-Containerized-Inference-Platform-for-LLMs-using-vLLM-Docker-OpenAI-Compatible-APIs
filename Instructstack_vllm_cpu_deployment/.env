# Environment Variables Template for VLLM CPU Deployment
# Copy this file to .env and modify the values as needed

# VLLM Server Configuration
VLLM_PORT=8000
CPU_DEVICE=cpu

# Model Configuration
DEFAULT_MODEL=facebook/opt-125m
DEFAULT_MODEL_1=sshleifer/tiny-gpt2

# Service Ports Configuration
FASTAPI_PORT=9000
PROMETHEUS_PORT=9090
NODE_EXPORTER_PORT=9100
GRAFANA_PORT=3000
CADVISOR_PORT=8081

# Container Names (optional - will use defaults if not set)
VLLM_CONTAINER_NAME=vllm_server
FASTAPI_CONTAINER_NAME=fastapi-app
PROMETHEUS_CONTAINER_NAME=prometheus
NODE_EXPORTER_CONTAINER_NAME=node_exporter
GRAFANA_CONTAINER_NAME=grafana
CADVISOR_CONTAINER_NAME=cadvisor

# Network Configuration
NETWORK_NAME=vllm-net

# FastAPI Configuration
VLLM_API_URL=http://vllm:8000/v1/completions
HOST_MODEL_PATH=/home/hamza/Instructstack/models

# Logfire Configuration
# Replace with your actual Logfire serve key
# LOGFIRE_TOKEN=your_logfire_token_here

# Model Download Configuration
MODEL_REPO_ID=sshleifer/tiny-gpt2
MODEL_LOCAL_DIR=models/sshleifer/tiny-gpt2
MODEL_USE_SYMLINKS=False

# Multiple Models Download (comma-separated)
# MODELS_TO_DOWNLOAD=model1,model2,model3
